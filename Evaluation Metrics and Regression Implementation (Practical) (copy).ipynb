{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "190d9677-1a96-47b5-b1c8-88c31761e835",
      "cell_type": "code",
      "source": "                  ### Evaluation Metrics and Regression Implementation Assignment (Pract) ###",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d237f8c3-a40c-4e8d-91a2-c5e605f320a4",
      "cell_type": "code",
      "source": "### Assignment Questions\n\n1. Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model using Seaborn's \"diamonds\" dataset\n2. Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) for a linear regression model\n3. Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity.\n4. Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the performance of different regression models\n5. Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and R-squared score.\n6. Write a Python script that analyzes the relationship between total bill and tip in the tips' dataset using simple linear regression and visualizes the results.\n7. Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the model to predict new values and plot the data points along with the regression line.\n8. Write a Python script that pickles a trained linear regression model and saves it to a file.\n9. Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the regression curve.\n10. Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear regression model to the data. Print the model's coefficient and intercept.\n11. Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and compares their performance.\n12. Write a Python script that fits a simple linear regression model with two features and prints the model's coefficients, intercept, and R-squared score.\n13. Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the regression line along with the data points.\n14. Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset with multiple features.\n15. Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a polynomial regression model, and plots the regression curve.\n16. Write a Python script that creates a machine learning pipeline with data standardization and a multiple linear regression model, and prints the R-squared score.\n17. Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the regression curve.\n18. Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print the R-squared score and model coefficients.\n19. Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the data points along with the regression line.\n20. Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's R-squared score and coefficients.\n21. Write a Python script that demonstrates how to serialize and deserialize machine learning models using joblib instead of pickling.\n22. Write a Python script to perform linear regression with categorical features using one-hot encoding. Use the Seaborn 'tips' dataset.\n23. Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and R-squared score.\n24. Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic dataset.\n25. Write a Python script that compares polynomial regression models of different degrees and prints the R-squared score for each.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d51aa420-df54-4616-9809-3049da435514",
      "cell_type": "code",
      "source": "Answer1:- import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Load diamonds dataset\ndiamonds = sns.load_dataset('diamonds')\n\n# Define features and target\nX = diamonds[['carat', 'depth', 'table']]\ny = diamonds['price']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = model.predict(X_test)\n\n# Calculate residuals\nresiduals = y_test - y_pred\n\n# Visualize distribution of residuals\nplt.figure(figsize=(8, 6))\nsns.histplot(residuals, kde=True)\nplt.title('Distribution of Residuals')\nplt.xlabel('Residuals')\nplt.ylabel('Frequency')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'SyntaxError'>",
          "evalue": "invalid syntax (<ipython-input-1-953c4c02a84b>, line 1)",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Answer1:- import seaborn as sns\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "53bec4b8-af83-41ae-a8c6-cff72e7087f3",
      "cell_type": "code",
      "source": "Answer2:- import numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Calculate MSE\nmse = mean_squared_error(y_test, y_pred)\n\n# Calculate MAE\nmae = mean_absolute_error(y_test, y_pred)\n\n# Calculate RMSE\nrmse = np.sqrt(mse)\n\nprint(f'MSE: {mse:.2f}')\nprint(f'MAE: {mae:.2f}')\nprint(f'RMSE: {rmse:.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'SyntaxError'>",
          "evalue": "invalid syntax (<ipython-input-2-f6fe5d56f41a>, line 1)",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Answer2:- import numpy as np\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "a9e7e3e4-fdd7-4a59-a56d-d3c2f41c9e0a",
      "cell_type": "code",
      "source": "Answer3:- import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Load dataset\ntips = sns.load_dataset('tips')\n\n# Define features and target\nX = tips[['total_bill']]\ny = tips['tip']\n\n# Fit linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Predict on dataset\ny_pred = model.predict(X)\n\n# Calculate residuals\nresiduals = y - y_pred\n\n# Check linearity\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X['total_bill'], y=y)\nplt.plot(X['total_bill'], y_pred, color='red')\nplt.title('Linearity Check')\nplt.xlabel('Total Bill')\nplt.ylabel('Tip')\nplt.show()\n\n# Check homoscedasticity\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=y_pred, y=residuals)\nplt.title('Homoscedasticity Check')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.show()\n\n# Check multicollinearity (not applicable for simple linear regression)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e9085697-270d-4ec7-934b-51b06cff6399",
      "cell_type": "code",
      "source": "Answer4:- from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load dataset\nfrom sklearn.datasets import make_regression\nX, y = make_regression(n_samples=100, n_features=5, random_state=42)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LinearRegression())\n])\n\n# Fit pipeline\npipeline.fit(X_train, y_train)\n\n# Evaluate pipeline\ny_pred = pipeline.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(f'MSE: {mse:.2f}')\n\n# Compare with RandomForestRegressor\npipeline_rf = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', RandomForestRegressor())\n])\npipeline_rf.fit(X_train, y_train)\ny_pred_rf = pipeline_rf.predict(X_test)\nmse_rf = mean_squared_error(y_test, y_pred_rf)\nprint(f'MSE (RandomForest): {mse_rf:.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dd240fa1-1966-4db2-8326-e927693e709a",
      "cell_type": "code",
      "source": "Answer5:- from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=100, n_features=1, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nprint(f'Coefficient: {model.coef_[0]:.2f}')\nprint(f'Intercept: {model.intercept_:.2f}')\nprint(f'R-squared: {model.score(X_test, y_test):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b930f1bf-7241-4462-9d6e-46b25910c7d5",
      "cell_type": "code",
      "source": "Answer6:- import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ntips = sns.load_dataset('tips')\nX = tips[['total_bill']]\ny = tips['tip']\n\nmodel = LinearRegression()\nmodel.fit(X, y)\ny_pred = model.predict(X)\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X['total_bill'], y=y)\nplt.plot(X['total_bill'], y_pred, color='red')\nplt.title('Relationship between Total Bill and Tip')\nplt.xlabel('Total Bill')\nplt.ylabel('Tip')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8d5354a3-99eb-48f5-b641-cb967c904ede",
      "cell_type": "code",
      "source": "Answer7:- import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate synthetic data\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = 3 * X + 2 + np.random.randn(100, 1)\n\n# Fit linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Predict new values\nX_test = np.linspace(0, 1, 100).reshape(-1, 1)\ny_pred = model.predict(X_test)\n\n# Plot data points and regression line\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y, label='Data Points')\nplt.plot(X_test, y_pred, color='red', label='Regression Line')\nplt.title('Linear Regression Model')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n# Print coefficients and intercept\nprint(f'Coefficient: {model.coef_[0]:.2f}')\nprint(f'Intercept: {model.intercept_:.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f2a1a97b-533a-4138-8689-0addf82c634b",
      "cell_type": "code",
      "source": "Answer8:- import pickle\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=100, n_features=1, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nwith open('model.pkl', 'wb') as f:\n    pickle.dump(model, f)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "69524779-54a3-4f4e-bb29-0e97eb5309ac",
      "cell_type": "code",
      "source": "Answer9:- import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = 3 * X**2 + 2 * X + 1 + np.random.randn(100, 1)\n\npoly_features = PolynomialFeatures(degree=2)\nX_poly = poly_features.fit_transform(X)\n\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\n\nX_test = np.linspace(0, 1, 100).reshape(-1, 1)\nX_test_poly = poly_features.transform(X_test)\ny_pred = model.predict(X_test_poly)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y)\nplt.plot(X_test, y_pred, color='red')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ec654cba-de7c-44fb-8364-b8a809758720",
      "cell_type": "code",
      "source": "Answer10:- import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = 3 * X + 2 + np.random.randn(100, 1)\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nprint(f'Coefficient: {model.coef_[0]:.2f}')\nprint(f'Intercept: {model.intercept_:.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ca872b07-8cb5-427d-9bcd-ebee9cdadf46",
      "cell_type": "code",
      "source": "Answer11:- import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = 3 * X**2 + 2 * X + 1 + np.random.randn(100, 1)\n\nfor degree in range(1, 4):\n    poly_features = PolynomialFeatures(degree=degree)\n    X_poly = poly_features.fit_transform(X)\n    model = LinearRegression()\n    model.fit(X_poly, y)\n    X_test = np.linspace(0, 1, 100).reshape(-1, 1)\n    X_test_poly = poly_features.transform(X_test)\n    y_pred = model.predict(X_test_poly)\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X, y)\n    plt.plot(X_test, y_pred, color='red')\n    plt.title(f'Degree {degree}')\n    plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a4ace347-617c-4436-8af5-f1686482a6f5",
      "cell_type": "code",
      "source": "Answer12:- import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(42)\nX = np.random.rand(100, 2)\ny = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(100)\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nprint(f'Coefficients: {model.coef_}')\nprint(f'Intercept: {model.intercept_}')\nprint(f'R-squared: {model.score(X, y):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5575e62d-2adb-49dc-8d83-30f68c7e69d1",
      "cell_type": "code",
      "source": "Answer13:- import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = 3 * X + 2 + np.random.randn(100, 1)\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nX_test = np.linspace(0, 1, 100).reshape(-1, 1)\ny_pred = model.predict(X_test)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y)\nplt.plot(X_test, y_pred, color='red')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d0ef5d9e-fd6c-4258-af4c-413fdf0b9d08",
      "cell_type": "code",
      "source": "Answer14:- from statsmodels.stats.outliers_influence import variance_inflation_factor\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\nX = np.random.rand(100, 5)\ndf = pd.DataFrame(X, columns=['feature1', 'feature2', 'feature3', 'feature4', 'feature5'])\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\nvif[\"features\"] = df.columns\n\nprint(vif)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0a37d632-5edb-4f12-9fff-e5dd3960850e",
      "cell_type": "code",
      "source": "Answer15:- import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = 3 * X**4 + 2 * X**3 + X**2 + X + np.random.randn(100, 1)\n\npoly_features = PolynomialFeatures(degree=4)\nX_poly = poly_features.fit_transform(X)\n\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\n\nX_test = np.linspace(0, 1, 100).reshape(-1, 1)\nX_test_poly = poly_features.transform(X_test)\ny_pred = model.predict(X_test_poly)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y)\nplt.plot(X_test, y_pred, color='red')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "546e3941-018f-4b0c-b164-54342bbb98b1",
      "cell_type": "code",
      "source": "Answer16:- from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=100, n_features=5, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LinearRegression())\n])\npipeline.fit(X_train, y_train)\n\nprint(f'R-squared: {pipeline.score(X_test, y_test):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c98c9bf4-ec87-4057-8486-e2a87051d603",
      "cell_type": "code",
      "source": "Answer17:- import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = 3 * X**3 + 2 * X**2 + X + np.random.randn(100, 1)\n\npoly_features = PolynomialFeatures(degree=3)\nX_poly = poly_features.fit_transform(X)\n\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\n\nX_test = np.linspace(0, 1, 100).reshape(-1, 1)\nX_test_poly = poly_features.transform(X_test)\ny_pred = model.predict(X_test_poly)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y)\nplt.plot(X_test, y_pred, color='red')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "64481ad2-6052-441d-923e-0c25562e48e5",
      "cell_type": "code",
      "source": "Answer18:- import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(42)\nX = np.random.rand(100, 5)\ny = 3 * X[:, 0] + 2 * X[:, 1] + X[:, 2] + X[:, 3] + X[:, 4] + np.random.randn(100)\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nprint(f'R-squared: {model.score(X, y):.2f}')\nprint(f'Coefficients: {model.coef_}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2cb51c73-facf-4bcd-891c-ed0a5f034d77",
      "cell_type": "code",
      "source": "Answer19:- import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = 3 * X + 2 + np.random.randn(100, 1)\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nX_test = np.linspace(0, 1, 100).reshape(-1, 1)\ny_pred = model.predict(X_test)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y)\nplt.plot(X_test, y_pred, color='red')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5a846126-d1f7-47d4-980a-1399b6f3eea5",
      "cell_type": "code",
      "source": "Answer20:- import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(42)\nX = np.random.rand(100, 3)\ny = 3 * X[:, 0] + 2 * X[:, 1] + X[:, 2] + np.random.randn(100)\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nprint(f'R-squared: {model.score(X, y):.2f}')\nprint(f'Coefficients: {model.coef_}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "54a31c61-8072-4428-b35e-f95d4fe7d4c0",
      "cell_type": "code",
      "source": "Answer21:- import joblib\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=100, n_features=5, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, 'model.joblib')\n\nloaded_model = joblib.load('model.joblib')\nprint(f'R-squared: {loaded_model.score(X_test, y_test):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "48110207-7b33-4ef8-8026-4c58f6d763e6",
      "cell_type": "code",
      "source": "Answer22:- import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nimport seaborn as sns\n\ntips = sns.load_dataset('tips')\nX = tips[['sex', 'smoker', 'day']]\ny = tips['total_bill']\n\nencoder = OneHotEncoder()\nX_encoded = encoder.fit_transform(X)\n\nmodel = LinearRegression()\nmodel.fit(X_encoded, y)\n\nprint(f'R-squared: {model.score(X_encoded, y):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "69c3f37e-9d2b-4c58-ab17-a124544ea079",
      "cell_type": "code",
      "source": "Answer23:- from sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=100, n_features=5, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel_lr = LinearRegression()\nmodel_lr.fit(X_train, y_train)\n\nmodel_ridge = Ridge()\nmodel_ridge.fit(X_train, y_train)\n\nprint(f'Linear Regression Coefficients: {model_lr.coef_}')\nprint(f'Ridge Regression Coefficients: {model_ridge.coef_}')\nprint(f'Linear Regression R-squared: {model_lr.score(X_test, y_test):.2f}')\nprint(f'Ridge Regression R-squared: {model_ridge.score(X_test, y_test):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bab46e21-2efa-4137-b48a-0fbb9af6469e",
      "cell_type": "code",
      "source": "Answer24:- from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=100, n_features=5, random_state=42)\n\nmodel = LinearRegression()\nscores = cross_val_score(model, X, y, cv=5)\n\nprint(f'Cross-validation scores: {scores}')\nprint(f'Average cross-validation score: {scores.mean():.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b3fadd6c-7d9b-4c34-8c2b-683282a97d23",
      "cell_type": "code",
      "source": "Answer25:- import numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = 3 * X**3 + 2 * X**2 + X + np.random.randn(100, 1)\n\nfor degree in range(1, 4):\n    poly_features = PolynomialFeatures(degree=degree)\n    X_poly = poly_features.fit_transform(X)\n    model = LinearRegression()\n    model.fit(X_poly, y)\n    print(f'Degree {degree} R-squared: {model.score(X_poly, y):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}